{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FGSM_CIFAR.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "SJ8S6JTYPNQq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "96273261-b741-441e-af87-f04c1330888d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "input_size = 1024*3\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data',        \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.reshape(-1, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 300 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/15], Step [300/500], Loss: 1.8697\n",
            "Epoch [2/15], Step [300/500], Loss: 1.6817\n",
            "Epoch [3/15], Step [300/500], Loss: 1.6816\n",
            "Epoch [4/15], Step [300/500], Loss: 1.5306\n",
            "Epoch [5/15], Step [300/500], Loss: 1.5579\n",
            "Epoch [6/15], Step [300/500], Loss: 1.5364\n",
            "Epoch [7/15], Step [300/500], Loss: 1.4349\n",
            "Epoch [8/15], Step [300/500], Loss: 1.4449\n",
            "Epoch [9/15], Step [300/500], Loss: 1.5286\n",
            "Epoch [10/15], Step [300/500], Loss: 1.5195\n",
            "Epoch [11/15], Step [300/500], Loss: 1.5488\n",
            "Epoch [12/15], Step [300/500], Loss: 1.3466\n",
            "Epoch [13/15], Step [300/500], Loss: 1.2682\n",
            "Epoch [14/15], Step [300/500], Loss: 1.2004\n",
            "Epoch [15/15], Step [300/500], Loss: 1.2825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Q0Rje5ZxPNQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "ea3b9a08-be64-487e-c926-7cbd7434f0bd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "\n",
        "class CIFAR10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10, self).__init__()\n",
        "        self.pad1 = nn.ZeroPad2d((2, 2, 2, 2))\n",
        "        \n",
        "        self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(7, 7), stride=(1, 1), bias=False), \n",
        "        nn.BatchNorm2d(num_features=32),\n",
        "        nn.MaxPool2d(kernel_size=(2,2)),\n",
        "        nn.ReLU()\n",
        "        )\n",
        "    \n",
        "        self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=(1, 1), bias=False), \n",
        "        nn.BatchNorm2d(num_features=64),\n",
        "        nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.layer3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), bias=False), \n",
        "        nn.BatchNorm2d(num_features=128),\n",
        "        nn.MaxPool2d(kernel_size=(3,3)),\n",
        "        nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Sequential(\n",
        "        nn.Linear(in_features=1152, out_features=512),\n",
        "        nn.ReLU()\n",
        "        )\n",
        "        self.dropout1 = nn.Dropout(p=0.2)\n",
        "        \n",
        "        self.fc2 = nn.Sequential(\n",
        "        nn.Linear(in_features=512, out_features=10),\n",
        "        nn.Softmax()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.pad1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout1(x)\n",
        "        out = self.fc2(x)\n",
        "        \n",
        "        return out\n",
        "      \n",
        "my_model = CIFAR10().to(device)\n",
        "summary(my_model, input_size=(3, 32, 32))  "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "         ZeroPad2d-1            [-1, 3, 36, 36]               0\n",
            "            Conv2d-2           [-1, 32, 30, 30]           4,704\n",
            "       BatchNorm2d-3           [-1, 32, 30, 30]              64\n",
            "         MaxPool2d-4           [-1, 32, 15, 15]               0\n",
            "              ReLU-5           [-1, 32, 15, 15]               0\n",
            "            Conv2d-6           [-1, 64, 11, 11]          51,200\n",
            "       BatchNorm2d-7           [-1, 64, 11, 11]             128\n",
            "              ReLU-8           [-1, 64, 11, 11]               0\n",
            "            Conv2d-9            [-1, 128, 9, 9]          73,728\n",
            "      BatchNorm2d-10            [-1, 128, 9, 9]             256\n",
            "        MaxPool2d-11            [-1, 128, 3, 3]               0\n",
            "             ReLU-12            [-1, 128, 3, 3]               0\n",
            "           Linear-13                  [-1, 512]         590,336\n",
            "             ReLU-14                  [-1, 512]               0\n",
            "          Dropout-15                  [-1, 512]               0\n",
            "           Linear-16                   [-1, 10]           5,130\n",
            "          Softmax-17                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 725,546\n",
            "Trainable params: 725,546\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.94\n",
            "Params size (MB): 2.77\n",
            "Estimated Total Size (MB): 3.72\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RBXh4MbqPNQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model = CIFAR10().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WGQh_h2CPNQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1329
        },
        "outputId": "5b79fa72-8c6d-43b5-ee44-f0231915e72f"
      },
      "source": [
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images_np = np.asarray(images)\n",
        "        images = images.reshape(-1, 3, 32, 32).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = my_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Step [100/500], Loss: 2.0212\n",
            "Epoch [1/15], Step [200/500], Loss: 2.0668\n",
            "Epoch [1/15], Step [300/500], Loss: 2.0412\n",
            "Epoch [1/15], Step [400/500], Loss: 2.0278\n",
            "Epoch [1/15], Step [500/500], Loss: 1.9098\n",
            "Epoch [2/15], Step [100/500], Loss: 1.9468\n",
            "Epoch [2/15], Step [200/500], Loss: 1.9195\n",
            "Epoch [2/15], Step [300/500], Loss: 2.0424\n",
            "Epoch [2/15], Step [400/500], Loss: 1.9922\n",
            "Epoch [2/15], Step [500/500], Loss: 1.9600\n",
            "Epoch [3/15], Step [100/500], Loss: 1.9038\n",
            "Epoch [3/15], Step [200/500], Loss: 1.8945\n",
            "Epoch [3/15], Step [300/500], Loss: 1.8352\n",
            "Epoch [3/15], Step [400/500], Loss: 1.8234\n",
            "Epoch [3/15], Step [500/500], Loss: 1.8969\n",
            "Epoch [4/15], Step [100/500], Loss: 1.8238\n",
            "Epoch [4/15], Step [200/500], Loss: 1.8464\n",
            "Epoch [4/15], Step [300/500], Loss: 1.8711\n",
            "Epoch [4/15], Step [400/500], Loss: 1.9298\n",
            "Epoch [4/15], Step [500/500], Loss: 1.8614\n",
            "Epoch [5/15], Step [100/500], Loss: 1.7542\n",
            "Epoch [5/15], Step [200/500], Loss: 1.7581\n",
            "Epoch [5/15], Step [300/500], Loss: 1.7489\n",
            "Epoch [5/15], Step [400/500], Loss: 1.8035\n",
            "Epoch [5/15], Step [500/500], Loss: 1.8984\n",
            "Epoch [6/15], Step [100/500], Loss: 1.7581\n",
            "Epoch [6/15], Step [200/500], Loss: 1.7656\n",
            "Epoch [6/15], Step [300/500], Loss: 1.7660\n",
            "Epoch [6/15], Step [400/500], Loss: 1.8999\n",
            "Epoch [6/15], Step [500/500], Loss: 1.8294\n",
            "Epoch [7/15], Step [100/500], Loss: 1.8588\n",
            "Epoch [7/15], Step [200/500], Loss: 1.8312\n",
            "Epoch [7/15], Step [300/500], Loss: 1.7674\n",
            "Epoch [7/15], Step [400/500], Loss: 1.7584\n",
            "Epoch [7/15], Step [500/500], Loss: 1.7762\n",
            "Epoch [8/15], Step [100/500], Loss: 1.8172\n",
            "Epoch [8/15], Step [200/500], Loss: 1.7615\n",
            "Epoch [8/15], Step [300/500], Loss: 1.7407\n",
            "Epoch [8/15], Step [400/500], Loss: 1.8472\n",
            "Epoch [8/15], Step [500/500], Loss: 1.8138\n",
            "Epoch [9/15], Step [100/500], Loss: 1.7474\n",
            "Epoch [9/15], Step [200/500], Loss: 1.7854\n",
            "Epoch [9/15], Step [300/500], Loss: 1.7435\n",
            "Epoch [9/15], Step [400/500], Loss: 1.7608\n",
            "Epoch [9/15], Step [500/500], Loss: 1.7536\n",
            "Epoch [10/15], Step [100/500], Loss: 1.7344\n",
            "Epoch [10/15], Step [200/500], Loss: 1.7209\n",
            "Epoch [10/15], Step [300/500], Loss: 1.7050\n",
            "Epoch [10/15], Step [500/500], Loss: 1.7839\n",
            "Epoch [11/15], Step [100/500], Loss: 1.7230\n",
            "Epoch [11/15], Step [200/500], Loss: 1.7244\n",
            "Epoch [11/15], Step [300/500], Loss: 1.6959\n",
            "Epoch [11/15], Step [400/500], Loss: 1.8150\n",
            "Epoch [11/15], Step [500/500], Loss: 1.7178\n",
            "Epoch [12/15], Step [100/500], Loss: 1.6907\n",
            "Epoch [12/15], Step [200/500], Loss: 1.6895\n",
            "Epoch [12/15], Step [300/500], Loss: 1.6927\n",
            "Epoch [12/15], Step [400/500], Loss: 1.7728\n",
            "Epoch [12/15], Step [500/500], Loss: 1.7932\n",
            "Epoch [13/15], Step [100/500], Loss: 1.6706\n",
            "Epoch [13/15], Step [200/500], Loss: 1.7496\n",
            "Epoch [13/15], Step [300/500], Loss: 1.6704\n",
            "Epoch [13/15], Step [400/500], Loss: 1.6823\n",
            "Epoch [13/15], Step [500/500], Loss: 1.7189\n",
            "Epoch [14/15], Step [100/500], Loss: 1.6518\n",
            "Epoch [14/15], Step [200/500], Loss: 1.7316\n",
            "Epoch [14/15], Step [300/500], Loss: 1.7630\n",
            "Epoch [14/15], Step [400/500], Loss: 1.7054\n",
            "Epoch [14/15], Step [500/500], Loss: 1.6471\n",
            "Epoch [15/15], Step [100/500], Loss: 1.7210\n",
            "Epoch [15/15], Step [200/500], Loss: 1.7296\n",
            "Epoch [15/15], Step [300/500], Loss: 1.6431\n",
            "Epoch [15/15], Step [400/500], Loss: 1.7110\n",
            "Epoch [15/15], Step [500/500], Loss: 1.6750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3fs7RfqlPNQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "fc3ed088-58bf-4d7e-957e-1837faa136d1"
      },
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 3, 32, 32).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = my_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 64.4 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TsXwukTTPNRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in my_model.parameters():\n",
        "    param.requires_grad= False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "p61gqyX_PNRE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c50410fd-d0c3-4d93-af19-325fb52a93d0"
      },
      "source": [
        "l =[]\n",
        "sign = []\n",
        "for epoch in range(1):\n",
        "    for i, (images, labels) in enumerate(test_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.reshape(-1, 3, 32, 32).to(device)\n",
        "        images.requires_grad=True\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = my_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "\n",
        "        loss.backward()\n",
        "        c =images.grad.sign()\n",
        "        images = images+ 0.01* c\n",
        "        sign.append(c)\n",
        "        l.append((images,labels))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "taLm4VrNPNRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im, _ = l[0]\n",
        "im = im.data.cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5iTY_3WWPNRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im = im.reshape(100, 3, 32, 32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "37muVykKPNRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "z3BLTBCXPNRQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "9ecef8c3-f6d6-4c01-d05f-d95157096792"
      },
      "source": [
        "plt.imshow(im[2, 0, :, :])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4c7e1ccef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRKYo+IXIlP8H/P4H2TrHJUUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iDKMQNCWPNRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c9a3fff8-b779-45d8-98d3-beb7ea75b3e7"
      },
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in l:\n",
        "        images = images.reshape(-1, 3, 32, 32).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = my_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 48.88 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iv6zlWjxPNRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}