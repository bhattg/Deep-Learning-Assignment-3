{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "SJ8S6JTYPNQq",
    "outputId": "8b7ccca7-96bf-42b9-8710-6f7c5011543f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/170498071 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 169779200/170498071 [00:10<00:00, 15296317.52it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 1024*3\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data',        \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "id": "Q0Rje5ZxPNQ0",
    "outputId": "fa656dff-d9c6-4d92-9fbc-a6573fdc5378"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "170500096it [00:29, 15296317.52it/s]                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         ZeroPad2d-1            [-1, 3, 36, 36]               0\n",
      "            Conv2d-2           [-1, 32, 30, 30]           4,704\n",
      "       BatchNorm2d-3           [-1, 32, 30, 30]              64\n",
      "         MaxPool2d-4           [-1, 32, 15, 15]               0\n",
      "              ReLU-5           [-1, 32, 15, 15]               0\n",
      "            Conv2d-6           [-1, 64, 11, 11]          51,200\n",
      "       BatchNorm2d-7           [-1, 64, 11, 11]             128\n",
      "              ReLU-8           [-1, 64, 11, 11]               0\n",
      "            Conv2d-9            [-1, 128, 9, 9]          73,728\n",
      "      BatchNorm2d-10            [-1, 128, 9, 9]             256\n",
      "        MaxPool2d-11            [-1, 128, 4, 4]               0\n",
      "             ReLU-12            [-1, 128, 4, 4]               0\n",
      "           Conv2d-13            [-1, 200, 2, 2]         230,400\n",
      "      BatchNorm2d-14            [-1, 200, 2, 2]             400\n",
      "             ReLU-15            [-1, 200, 2, 2]               0\n",
      "           Linear-16                  [-1, 512]         410,112\n",
      "             ReLU-17                  [-1, 512]               0\n",
      "          Dropout-18                  [-1, 512]               0\n",
      "           Linear-19                   [-1, 10]           5,130\n",
      "          Softmax-20                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 776,122\n",
      "Trainable params: 776,122\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.98\n",
      "Params size (MB): 2.96\n",
      "Estimated Total Size (MB): 3.95\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class CIFAR10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10, self).__init__()\n",
    "        self.pad1 = nn.ZeroPad2d((2, 2, 2, 2))\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(7, 7), stride=(1, 1), bias=False), \n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.MaxPool2d(kernel_size=(2,2)),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "    \n",
    "        self.layer2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=(1, 1), bias=False), \n",
    "        nn.BatchNorm2d(num_features=64),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), bias=False), \n",
    "        nn.BatchNorm2d(num_features=128),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=128, out_channels=200, kernel_size=(3, 3), stride=(1, 1), bias=False), \n",
    "        nn.BatchNorm2d(num_features=200),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "        nn.Linear(in_features=800, out_features=512),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "        nn.Linear(in_features=512, out_features=10),\n",
    "        nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pad1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "my_model = CIFAR10().to(device)\n",
    "summary(my_model, input_size=(3, 32, 32))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RBXh4MbqPNQ3",
    "outputId": "e3d0f0ba-6453-49b6-dbe3-e22589205c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "my_model = CIFAR10().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=0.01) \n",
    "print(\"Learning rate: \", learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 8470
    },
    "colab_type": "code",
    "id": "WGQh_h2CPNQ6",
    "outputId": "5ad17307-6255-4f29-b4d3-1761186b9787"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/500], Loss: 2.1310\n",
      "Epoch [1/100], Step [200/500], Loss: 2.2213\n",
      "Epoch [1/100], Step [300/500], Loss: 2.1724\n",
      "Epoch [1/100], Step [400/500], Loss: 2.1313\n",
      "Epoch [1/100], Step [500/500], Loss: 2.2270\n",
      "Epoch [2/100], Step [100/500], Loss: 2.2110\n",
      "Epoch [2/100], Step [200/500], Loss: 2.0950\n",
      "Epoch [2/100], Step [300/500], Loss: 2.1802\n",
      "Epoch [2/100], Step [400/500], Loss: 2.2682\n",
      "Epoch [2/100], Step [500/500], Loss: 2.2088\n",
      "Epoch [3/100], Step [100/500], Loss: 2.1941\n",
      "Epoch [3/100], Step [200/500], Loss: 2.2702\n",
      "Epoch [3/100], Step [300/500], Loss: 2.2306\n",
      "Epoch [3/100], Step [400/500], Loss: 2.2409\n",
      "Epoch [3/100], Step [500/500], Loss: 2.1511\n",
      "Epoch [4/100], Step [100/500], Loss: 2.1820\n",
      "Epoch [4/100], Step [200/500], Loss: 2.1435\n",
      "Epoch [4/100], Step [300/500], Loss: 2.1804\n",
      "Epoch [4/100], Step [400/500], Loss: 2.1512\n",
      "Epoch [4/100], Step [500/500], Loss: 2.2217\n",
      "Epoch [5/100], Step [100/500], Loss: 2.1396\n",
      "Epoch [5/100], Step [200/500], Loss: 2.1739\n",
      "Epoch [5/100], Step [300/500], Loss: 2.2605\n",
      "Epoch [5/100], Step [400/500], Loss: 2.2307\n",
      "Epoch [5/100], Step [500/500], Loss: 2.1811\n",
      "Epoch [6/100], Step [100/500], Loss: 2.1653\n",
      "Epoch [6/100], Step [200/500], Loss: 2.1998\n",
      "Epoch [6/100], Step [300/500], Loss: 2.2211\n",
      "Epoch [6/100], Step [400/500], Loss: 2.1811\n",
      "Epoch [6/100], Step [500/500], Loss: 2.2673\n",
      "Epoch [7/100], Step [100/500], Loss: 2.2512\n",
      "Epoch [7/100], Step [200/500], Loss: 2.2506\n",
      "Epoch [7/100], Step [300/500], Loss: 2.1611\n",
      "Epoch [7/100], Step [400/500], Loss: 2.1812\n",
      "Epoch [7/100], Step [500/500], Loss: 2.1808\n",
      "Epoch [8/100], Step [100/500], Loss: 2.1812\n",
      "Epoch [8/100], Step [200/500], Loss: 2.0910\n",
      "Epoch [8/100], Step [300/500], Loss: 2.0809\n",
      "Epoch [8/100], Step [400/500], Loss: 2.1414\n",
      "Epoch [8/100], Step [500/500], Loss: 2.1512\n",
      "Epoch [9/100], Step [100/500], Loss: 2.2611\n",
      "Epoch [9/100], Step [200/500], Loss: 2.1614\n",
      "Epoch [9/100], Step [300/500], Loss: 2.2511\n",
      "Epoch [9/100], Step [400/500], Loss: 2.2010\n",
      "Epoch [9/100], Step [500/500], Loss: 2.1712\n",
      "Epoch [10/100], Step [100/500], Loss: 2.1909\n",
      "Epoch [10/100], Step [200/500], Loss: 2.1810\n",
      "Epoch [10/100], Step [300/500], Loss: 2.1712\n",
      "Epoch [10/100], Step [400/500], Loss: 2.2009\n",
      "Epoch [10/100], Step [500/500], Loss: 2.3111\n",
      "Epoch [11/100], Step [100/500], Loss: 2.2711\n",
      "Epoch [11/100], Step [200/500], Loss: 2.1232\n",
      "Epoch [11/100], Step [300/500], Loss: 2.1705\n",
      "Epoch [11/100], Step [400/500], Loss: 2.2112\n",
      "Epoch [11/100], Step [500/500], Loss: 2.1612\n",
      "Epoch [12/100], Step [100/500], Loss: 2.1812\n",
      "Epoch [12/100], Step [200/500], Loss: 2.2611\n",
      "Epoch [12/100], Step [300/500], Loss: 2.2206\n",
      "Epoch [12/100], Step [400/500], Loss: 2.2111\n",
      "Epoch [12/100], Step [500/500], Loss: 2.1312\n",
      "Epoch [13/100], Step [100/500], Loss: 2.2211\n",
      "Epoch [13/100], Step [200/500], Loss: 2.1712\n",
      "Epoch [13/100], Step [300/500], Loss: 2.3112\n",
      "Epoch [13/100], Step [400/500], Loss: 2.1512\n",
      "Epoch [13/100], Step [500/500], Loss: 2.2211\n",
      "Epoch [14/100], Step [100/500], Loss: 2.2012\n",
      "Epoch [14/100], Step [200/500], Loss: 2.2312\n",
      "Epoch [14/100], Step [300/500], Loss: 2.2112\n",
      "Epoch [14/100], Step [400/500], Loss: 2.1712\n",
      "Epoch [14/100], Step [500/500], Loss: 2.2511\n",
      "Epoch [15/100], Step [100/500], Loss: 2.1909\n",
      "Epoch [15/100], Step [200/500], Loss: 2.2510\n",
      "Epoch [15/100], Step [300/500], Loss: 2.2911\n",
      "Epoch [15/100], Step [400/500], Loss: 2.1611\n",
      "Epoch [15/100], Step [500/500], Loss: 2.2812\n",
      "Epoch [16/100], Step [100/500], Loss: 2.2459\n",
      "Epoch [16/100], Step [200/500], Loss: 2.2712\n",
      "Epoch [16/100], Step [300/500], Loss: 2.1911\n",
      "Epoch [16/100], Step [400/500], Loss: 2.2209\n",
      "Epoch [16/100], Step [500/500], Loss: 2.0908\n",
      "Epoch [17/100], Step [100/500], Loss: 2.2812\n",
      "Epoch [17/100], Step [200/500], Loss: 2.1909\n",
      "Epoch [17/100], Step [300/500], Loss: 2.2012\n",
      "Epoch [17/100], Step [400/500], Loss: 2.2312\n",
      "Epoch [17/100], Step [500/500], Loss: 2.2112\n",
      "Epoch [18/100], Step [100/500], Loss: 2.2212\n",
      "Epoch [18/100], Step [200/500], Loss: 2.2409\n",
      "Epoch [18/100], Step [300/500], Loss: 2.1912\n",
      "Epoch [18/100], Step [400/500], Loss: 2.2312\n",
      "Epoch [18/100], Step [500/500], Loss: 2.2512\n",
      "Epoch [19/100], Step [100/500], Loss: 2.2312\n",
      "Epoch [19/100], Step [200/500], Loss: 2.2812\n",
      "Epoch [19/100], Step [300/500], Loss: 2.2612\n",
      "Epoch [19/100], Step [400/500], Loss: 2.1811\n",
      "Epoch [19/100], Step [500/500], Loss: 2.1898\n",
      "Epoch [20/100], Step [100/500], Loss: 2.1410\n",
      "Epoch [20/100], Step [200/500], Loss: 2.2312\n",
      "Epoch [20/100], Step [300/500], Loss: 2.1852\n",
      "Epoch [20/100], Step [400/500], Loss: 2.1810\n",
      "Epoch [20/100], Step [500/500], Loss: 2.2112\n",
      "Epoch [21/100], Step [100/500], Loss: 2.1221\n",
      "Epoch [21/100], Step [200/500], Loss: 2.2108\n",
      "Epoch [21/100], Step [300/500], Loss: 2.1511\n",
      "Epoch [21/100], Step [400/500], Loss: 2.2012\n",
      "Epoch [21/100], Step [500/500], Loss: 2.2011\n",
      "Epoch [22/100], Step [100/500], Loss: 2.2111\n",
      "Epoch [22/100], Step [200/500], Loss: 2.2612\n",
      "Epoch [22/100], Step [300/500], Loss: 2.2512\n",
      "Epoch [22/100], Step [400/500], Loss: 2.2012\n",
      "Epoch [22/100], Step [500/500], Loss: 2.2312\n",
      "Epoch [23/100], Step [100/500], Loss: 2.1212\n",
      "Epoch [23/100], Step [200/500], Loss: 2.2312\n",
      "Epoch [23/100], Step [300/500], Loss: 2.2312\n",
      "Epoch [23/100], Step [400/500], Loss: 2.1567\n",
      "Epoch [23/100], Step [500/500], Loss: 2.1912\n",
      "Epoch [24/100], Step [100/500], Loss: 2.1911\n",
      "Epoch [24/100], Step [200/500], Loss: 2.1711\n",
      "Epoch [24/100], Step [300/500], Loss: 2.1450\n",
      "Epoch [24/100], Step [400/500], Loss: 2.1812\n",
      "Epoch [24/100], Step [500/500], Loss: 2.1812\n",
      "Epoch [25/100], Step [100/500], Loss: 2.1712\n",
      "Epoch [25/100], Step [200/500], Loss: 2.1912\n",
      "Epoch [25/100], Step [300/500], Loss: 2.1709\n",
      "Epoch [25/100], Step [400/500], Loss: 2.1512\n",
      "Epoch [25/100], Step [500/500], Loss: 2.2709\n",
      "Epoch [26/100], Step [100/500], Loss: 2.2212\n",
      "Epoch [26/100], Step [200/500], Loss: 2.1812\n",
      "Epoch [26/100], Step [300/500], Loss: 2.2111\n",
      "Epoch [26/100], Step [400/500], Loss: 2.1312\n",
      "Epoch [26/100], Step [500/500], Loss: 2.1211\n",
      "Epoch [27/100], Step [100/500], Loss: 2.2312\n",
      "Epoch [27/100], Step [200/500], Loss: 2.1312\n",
      "Epoch [27/100], Step [300/500], Loss: 2.1611\n",
      "Epoch [27/100], Step [400/500], Loss: 2.2312\n",
      "Epoch [27/100], Step [500/500], Loss: 2.2111\n",
      "Epoch [28/100], Step [100/500], Loss: 2.2612\n",
      "Epoch [28/100], Step [200/500], Loss: 2.3111\n",
      "Epoch [28/100], Step [300/500], Loss: 2.2712\n",
      "Epoch [28/100], Step [400/500], Loss: 2.2611\n",
      "Epoch [28/100], Step [500/500], Loss: 2.2812\n",
      "Epoch [29/100], Step [100/500], Loss: 2.2612\n",
      "Epoch [29/100], Step [200/500], Loss: 2.2201\n",
      "Epoch [29/100], Step [300/500], Loss: 2.1909\n",
      "Epoch [29/100], Step [400/500], Loss: 2.2012\n",
      "Epoch [29/100], Step [500/500], Loss: 2.2009\n",
      "Epoch [30/100], Step [100/500], Loss: 2.1912\n",
      "Epoch [30/100], Step [200/500], Loss: 2.2511\n",
      "Epoch [30/100], Step [300/500], Loss: 2.1812\n",
      "Epoch [30/100], Step [400/500], Loss: 2.2912\n",
      "Epoch [30/100], Step [500/500], Loss: 2.2412\n",
      "Epoch [31/100], Step [100/500], Loss: 2.2212\n",
      "Epoch [31/100], Step [200/500], Loss: 2.1311\n",
      "Epoch [31/100], Step [300/500], Loss: 2.1412\n",
      "Epoch [31/100], Step [400/500], Loss: 2.2412\n",
      "Epoch [31/100], Step [500/500], Loss: 2.2812\n",
      "Epoch [32/100], Step [100/500], Loss: 2.2012\n",
      "Epoch [32/100], Step [200/500], Loss: 2.1412\n",
      "Epoch [32/100], Step [300/500], Loss: 2.1612\n",
      "Epoch [32/100], Step [400/500], Loss: 2.2312\n",
      "Epoch [32/100], Step [500/500], Loss: 2.1612\n",
      "Epoch [33/100], Step [100/500], Loss: 2.1812\n",
      "Epoch [33/100], Step [200/500], Loss: 2.2112\n",
      "Epoch [33/100], Step [300/500], Loss: 2.2512\n",
      "Epoch [33/100], Step [400/500], Loss: 2.1612\n",
      "Epoch [33/100], Step [500/500], Loss: 2.2411\n",
      "Epoch [34/100], Step [100/500], Loss: 2.2010\n",
      "Epoch [34/100], Step [200/500], Loss: 2.2612\n",
      "Epoch [34/100], Step [300/500], Loss: 2.1812\n",
      "Epoch [34/100], Step [400/500], Loss: 2.1811\n",
      "Epoch [34/100], Step [500/500], Loss: 2.1412\n",
      "Epoch [35/100], Step [100/500], Loss: 2.2134\n",
      "Epoch [35/100], Step [200/500], Loss: 2.2412\n",
      "Epoch [35/100], Step [300/500], Loss: 2.1511\n",
      "Epoch [35/100], Step [400/500], Loss: 2.1912\n",
      "Epoch [35/100], Step [500/500], Loss: 2.2512\n",
      "Epoch [36/100], Step [100/500], Loss: 2.2311\n",
      "Epoch [36/100], Step [200/500], Loss: 2.2511\n",
      "Epoch [36/100], Step [300/500], Loss: 2.0912\n",
      "Epoch [36/100], Step [400/500], Loss: 2.2712\n",
      "Epoch [36/100], Step [500/500], Loss: 2.1412\n",
      "Epoch [37/100], Step [100/500], Loss: 2.2412\n",
      "Epoch [37/100], Step [200/500], Loss: 2.2112\n",
      "Epoch [37/100], Step [300/500], Loss: 2.2312\n",
      "Epoch [37/100], Step [400/500], Loss: 2.2012\n",
      "Epoch [37/100], Step [500/500], Loss: 2.1808\n",
      "Epoch [38/100], Step [100/500], Loss: 2.2212\n",
      "Epoch [38/100], Step [200/500], Loss: 2.1811\n",
      "Epoch [38/100], Step [300/500], Loss: 2.1612\n",
      "Epoch [38/100], Step [400/500], Loss: 2.2212\n",
      "Epoch [38/100], Step [500/500], Loss: 2.1812\n",
      "Epoch [39/100], Step [100/500], Loss: 2.1808\n",
      "Epoch [39/100], Step [200/500], Loss: 2.2712\n",
      "Epoch [39/100], Step [300/500], Loss: 2.2408\n",
      "Epoch [39/100], Step [400/500], Loss: 2.1812\n",
      "Epoch [39/100], Step [500/500], Loss: 2.1912\n",
      "Epoch [40/100], Step [100/500], Loss: 2.1912\n",
      "Epoch [40/100], Step [200/500], Loss: 2.1914\n",
      "Epoch [40/100], Step [300/500], Loss: 2.2012\n",
      "Epoch [40/100], Step [400/500], Loss: 2.2364\n",
      "Epoch [40/100], Step [500/500], Loss: 2.2112\n",
      "Epoch [41/100], Step [100/500], Loss: 2.2309\n",
      "Epoch [41/100], Step [200/500], Loss: 2.1912\n",
      "Epoch [41/100], Step [300/500], Loss: 2.1909\n",
      "Epoch [41/100], Step [400/500], Loss: 2.2012\n",
      "Epoch [41/100], Step [500/500], Loss: 2.2110\n",
      "Epoch [42/100], Step [100/500], Loss: 2.1911\n",
      "Epoch [42/100], Step [200/500], Loss: 2.2212\n",
      "Epoch [42/100], Step [300/500], Loss: 2.2711\n",
      "Epoch [42/100], Step [400/500], Loss: 2.2312\n",
      "Epoch [42/100], Step [500/500], Loss: 2.1209\n",
      "Epoch [43/100], Step [100/500], Loss: 2.2412\n",
      "Epoch [43/100], Step [200/500], Loss: 2.1708\n",
      "Epoch [43/100], Step [300/500], Loss: 2.1807\n",
      "Epoch [43/100], Step [400/500], Loss: 2.1812\n",
      "Epoch [43/100], Step [500/500], Loss: 2.1012\n",
      "Epoch [44/100], Step [100/500], Loss: 2.1612\n",
      "Epoch [44/100], Step [200/500], Loss: 2.1312\n",
      "Epoch [44/100], Step [300/500], Loss: 2.1812\n",
      "Epoch [44/100], Step [400/500], Loss: 2.1612\n",
      "Epoch [44/100], Step [500/500], Loss: 2.1612\n",
      "Epoch [45/100], Step [100/500], Loss: 2.2311\n",
      "Epoch [45/100], Step [200/500], Loss: 2.2012\n",
      "Epoch [45/100], Step [300/500], Loss: 2.1411\n",
      "Epoch [45/100], Step [400/500], Loss: 2.2512\n",
      "Epoch [45/100], Step [500/500], Loss: 2.2412\n",
      "Epoch [46/100], Step [100/500], Loss: 2.2212\n",
      "Epoch [46/100], Step [200/500], Loss: 2.2512\n",
      "Epoch [46/100], Step [300/500], Loss: 2.2812\n",
      "Epoch [46/100], Step [400/500], Loss: 2.2411\n",
      "Epoch [46/100], Step [500/500], Loss: 2.2712\n",
      "Epoch [47/100], Step [100/500], Loss: 2.2310\n",
      "Epoch [47/100], Step [200/500], Loss: 2.2212\n",
      "Epoch [47/100], Step [300/500], Loss: 2.2012\n",
      "Epoch [47/100], Step [400/500], Loss: 2.2512\n",
      "Epoch [47/100], Step [500/500], Loss: 2.1812\n",
      "Epoch [48/100], Step [100/500], Loss: 2.1412\n",
      "Epoch [48/100], Step [200/500], Loss: 2.1112\n",
      "Epoch [48/100], Step [300/500], Loss: 2.2112\n",
      "Epoch [48/100], Step [400/500], Loss: 2.2112\n",
      "Epoch [48/100], Step [500/500], Loss: 2.1512\n",
      "Epoch [49/100], Step [100/500], Loss: 2.1812\n",
      "Epoch [49/100], Step [200/500], Loss: 2.1812\n",
      "Epoch [49/100], Step [300/500], Loss: 2.3212\n",
      "Epoch [49/100], Step [400/500], Loss: 2.1811\n",
      "Epoch [49/100], Step [500/500], Loss: 2.1911\n",
      "Epoch [50/100], Step [100/500], Loss: 2.2312\n",
      "Epoch [50/100], Step [200/500], Loss: 2.2612\n",
      "Epoch [50/100], Step [300/500], Loss: 2.3312\n",
      "Epoch [50/100], Step [400/500], Loss: 2.2612\n",
      "Epoch [50/100], Step [500/500], Loss: 2.1712\n",
      "Epoch [51/100], Step [100/500], Loss: 2.1711\n",
      "Epoch [51/100], Step [200/500], Loss: 2.1812\n",
      "Epoch [51/100], Step [300/500], Loss: 2.1911\n",
      "Epoch [51/100], Step [400/500], Loss: 2.2112\n",
      "Epoch [51/100], Step [500/500], Loss: 2.1212\n",
      "Epoch [52/100], Step [100/500], Loss: 2.1712\n",
      "Epoch [52/100], Step [200/500], Loss: 2.1509\n",
      "Epoch [52/100], Step [300/500], Loss: 2.1312\n",
      "Epoch [52/100], Step [400/500], Loss: 2.1312\n",
      "Epoch [52/100], Step [500/500], Loss: 2.1412\n",
      "Epoch [53/100], Step [100/500], Loss: 2.1412\n",
      "Epoch [53/100], Step [200/500], Loss: 2.1712\n",
      "Epoch [53/100], Step [300/500], Loss: 2.1812\n",
      "Epoch [53/100], Step [400/500], Loss: 2.1412\n",
      "Epoch [53/100], Step [500/500], Loss: 2.1912\n",
      "Epoch [54/100], Step [100/500], Loss: 2.1512\n",
      "Epoch [54/100], Step [200/500], Loss: 2.1411\n",
      "Epoch [54/100], Step [300/500], Loss: 2.1109\n",
      "Epoch [54/100], Step [400/500], Loss: 2.1512\n",
      "Epoch [54/100], Step [500/500], Loss: 2.1603\n",
      "Epoch [55/100], Step [100/500], Loss: 2.1512\n",
      "Epoch [55/100], Step [200/500], Loss: 2.1609\n",
      "Epoch [55/100], Step [300/500], Loss: 2.1612\n",
      "Epoch [55/100], Step [400/500], Loss: 2.1409\n",
      "Epoch [55/100], Step [500/500], Loss: 2.2012\n",
      "Epoch [56/100], Step [100/500], Loss: 2.1812\n",
      "Epoch [56/100], Step [200/500], Loss: 2.1812\n",
      "Epoch [56/100], Step [300/500], Loss: 2.2012\n",
      "Epoch [56/100], Step [400/500], Loss: 2.2512\n",
      "Epoch [56/100], Step [500/500], Loss: 2.2212\n",
      "Epoch [57/100], Step [100/500], Loss: 2.2011\n",
      "Epoch [57/100], Step [200/500], Loss: 2.2212\n",
      "Epoch [57/100], Step [300/500], Loss: 2.1212\n",
      "Epoch [57/100], Step [400/500], Loss: 2.1312\n",
      "Epoch [57/100], Step [500/500], Loss: 2.2012\n",
      "Epoch [58/100], Step [100/500], Loss: 2.1712\n",
      "Epoch [58/100], Step [200/500], Loss: 2.1611\n",
      "Epoch [58/100], Step [300/500], Loss: 2.1911\n",
      "Epoch [58/100], Step [400/500], Loss: 2.1112\n",
      "Epoch [58/100], Step [500/500], Loss: 2.2912\n",
      "Epoch [59/100], Step [100/500], Loss: 2.1312\n",
      "Epoch [59/100], Step [200/500], Loss: 2.0912\n",
      "Epoch [59/100], Step [300/500], Loss: 2.1911\n",
      "Epoch [59/100], Step [400/500], Loss: 2.2312\n",
      "Epoch [59/100], Step [500/500], Loss: 2.2112\n",
      "Epoch [60/100], Step [100/500], Loss: 2.1908\n",
      "Epoch [60/100], Step [200/500], Loss: 2.1612\n",
      "Epoch [60/100], Step [300/500], Loss: 2.1512\n",
      "Epoch [60/100], Step [400/500], Loss: 2.1512\n",
      "Epoch [60/100], Step [500/500], Loss: 2.2212\n",
      "Epoch [61/100], Step [100/500], Loss: 2.1512\n",
      "Epoch [61/100], Step [200/500], Loss: 2.2110\n",
      "Epoch [61/100], Step [300/500], Loss: 2.1912\n",
      "Epoch [61/100], Step [400/500], Loss: 2.1912\n",
      "Epoch [61/100], Step [500/500], Loss: 2.1712\n",
      "Epoch [62/100], Step [100/500], Loss: 2.2011\n",
      "Epoch [62/100], Step [200/500], Loss: 2.0912\n",
      "Epoch [62/100], Step [300/500], Loss: 2.2312\n",
      "Epoch [62/100], Step [400/500], Loss: 2.2612\n",
      "Epoch [62/100], Step [500/500], Loss: 2.2012\n",
      "Epoch [63/100], Step [100/500], Loss: 2.1412\n",
      "Epoch [63/100], Step [200/500], Loss: 2.1712\n",
      "Epoch [63/100], Step [300/500], Loss: 2.2712\n",
      "Epoch [63/100], Step [400/500], Loss: 2.2212\n",
      "Epoch [63/100], Step [500/500], Loss: 2.1912\n",
      "Epoch [64/100], Step [100/500], Loss: 2.2812\n",
      "Epoch [64/100], Step [200/500], Loss: 2.2112\n",
      "Epoch [64/100], Step [300/500], Loss: 2.2211\n",
      "Epoch [64/100], Step [400/500], Loss: 2.2712\n",
      "Epoch [64/100], Step [500/500], Loss: 2.1612\n",
      "Epoch [65/100], Step [100/500], Loss: 2.1912\n",
      "Epoch [65/100], Step [200/500], Loss: 2.2012\n",
      "Epoch [65/100], Step [300/500], Loss: 2.1912\n",
      "Epoch [65/100], Step [400/500], Loss: 2.2612\n",
      "Epoch [65/100], Step [500/500], Loss: 2.1912\n",
      "Epoch [66/100], Step [100/500], Loss: 2.2612\n",
      "Epoch [66/100], Step [200/500], Loss: 2.2312\n",
      "Epoch [66/100], Step [300/500], Loss: 2.2112\n",
      "Epoch [66/100], Step [400/500], Loss: 2.2412\n",
      "Epoch [66/100], Step [500/500], Loss: 2.2512\n",
      "Epoch [67/100], Step [100/500], Loss: 2.2312\n",
      "Epoch [67/100], Step [200/500], Loss: 2.2112\n",
      "Epoch [67/100], Step [300/500], Loss: 2.2012\n",
      "Epoch [67/100], Step [400/500], Loss: 2.2212\n",
      "Epoch [67/100], Step [500/500], Loss: 2.2012\n",
      "Epoch [68/100], Step [100/500], Loss: 2.3112\n",
      "Epoch [68/100], Step [200/500], Loss: 2.2512\n",
      "Epoch [68/100], Step [300/500], Loss: 2.2512\n",
      "Epoch [68/100], Step [400/500], Loss: 2.1112\n",
      "Epoch [68/100], Step [500/500], Loss: 2.1712\n",
      "Epoch [69/100], Step [100/500], Loss: 2.2612\n",
      "Epoch [69/100], Step [200/500], Loss: 2.2211\n",
      "Epoch [69/100], Step [300/500], Loss: 2.1912\n",
      "Epoch [69/100], Step [400/500], Loss: 2.3012\n",
      "Epoch [69/100], Step [500/500], Loss: 2.2212\n",
      "Epoch [70/100], Step [100/500], Loss: 2.2312\n",
      "Epoch [70/100], Step [200/500], Loss: 2.2112\n",
      "Epoch [70/100], Step [300/500], Loss: 2.2612\n",
      "Epoch [70/100], Step [400/500], Loss: 2.2312\n",
      "Epoch [70/100], Step [500/500], Loss: 2.2211\n",
      "Epoch [71/100], Step [100/500], Loss: 2.2212\n",
      "Epoch [71/100], Step [200/500], Loss: 2.2912\n",
      "Epoch [71/100], Step [300/500], Loss: 2.2912\n",
      "Epoch [71/100], Step [400/500], Loss: 2.2012\n",
      "Epoch [71/100], Step [500/500], Loss: 2.2712\n",
      "Epoch [72/100], Step [100/500], Loss: 2.1712\n",
      "Epoch [72/100], Step [200/500], Loss: 2.2212\n",
      "Epoch [72/100], Step [300/500], Loss: 2.1812\n",
      "Epoch [72/100], Step [400/500], Loss: 2.2012\n",
      "Epoch [72/100], Step [500/500], Loss: 2.1512\n",
      "Epoch [73/100], Step [100/500], Loss: 2.2412\n",
      "Epoch [73/100], Step [200/500], Loss: 2.2212\n",
      "Epoch [73/100], Step [300/500], Loss: 2.2212\n",
      "Epoch [73/100], Step [400/500], Loss: 2.2012\n",
      "Epoch [73/100], Step [500/500], Loss: 2.2312\n",
      "Epoch [74/100], Step [100/500], Loss: 2.2412\n",
      "Epoch [74/100], Step [200/500], Loss: 2.1012\n",
      "Epoch [74/100], Step [300/500], Loss: 2.2212\n",
      "Epoch [74/100], Step [400/500], Loss: 2.2312\n",
      "Epoch [74/100], Step [500/500], Loss: 2.1812\n",
      "Epoch [75/100], Step [100/500], Loss: 2.1912\n",
      "Epoch [75/100], Step [200/500], Loss: 2.2712\n",
      "Epoch [75/100], Step [300/500], Loss: 2.2212\n",
      "Epoch [75/100], Step [400/500], Loss: 2.2212\n",
      "Epoch [75/100], Step [500/500], Loss: 2.1812\n",
      "Epoch [76/100], Step [100/500], Loss: 2.2109\n",
      "Epoch [76/100], Step [200/500], Loss: 2.1812\n",
      "Epoch [76/100], Step [300/500], Loss: 2.1412\n",
      "Epoch [76/100], Step [400/500], Loss: 2.1312\n",
      "Epoch [76/100], Step [500/500], Loss: 2.2412\n",
      "Epoch [77/100], Step [100/500], Loss: 2.1012\n",
      "Epoch [77/100], Step [200/500], Loss: 2.1312\n",
      "Epoch [77/100], Step [300/500], Loss: 2.1812\n",
      "Epoch [77/100], Step [400/500], Loss: 2.1012\n",
      "Epoch [77/100], Step [500/500], Loss: 2.1812\n",
      "Epoch [78/100], Step [100/500], Loss: 2.1412\n",
      "Epoch [78/100], Step [200/500], Loss: 2.2112\n",
      "Epoch [78/100], Step [300/500], Loss: 2.1312\n",
      "Epoch [78/100], Step [400/500], Loss: 2.2412\n",
      "Epoch [78/100], Step [500/500], Loss: 2.1712\n",
      "Epoch [79/100], Step [100/500], Loss: 2.2412\n",
      "Epoch [79/100], Step [200/500], Loss: 2.3212\n",
      "Epoch [79/100], Step [300/500], Loss: 2.2411\n",
      "Epoch [79/100], Step [400/500], Loss: 2.1809\n",
      "Epoch [79/100], Step [500/500], Loss: 2.2012\n",
      "Epoch [80/100], Step [100/500], Loss: 2.3412\n",
      "Epoch [80/100], Step [200/500], Loss: 2.2511\n",
      "Epoch [80/100], Step [300/500], Loss: 2.2912\n",
      "Epoch [80/100], Step [400/500], Loss: 2.2012\n",
      "Epoch [80/100], Step [500/500], Loss: 2.2412\n",
      "Epoch [81/100], Step [100/500], Loss: 2.2712\n",
      "Epoch [81/100], Step [200/500], Loss: 2.1812\n",
      "Epoch [81/100], Step [300/500], Loss: 2.2412\n",
      "Epoch [81/100], Step [400/500], Loss: 2.2912\n",
      "Epoch [81/100], Step [500/500], Loss: 2.2812\n",
      "Epoch [82/100], Step [100/500], Loss: 2.2912\n",
      "Epoch [82/100], Step [200/500], Loss: 2.2412\n",
      "Epoch [82/100], Step [300/500], Loss: 2.2012\n",
      "Epoch [82/100], Step [400/500], Loss: 2.2412\n",
      "Epoch [82/100], Step [500/500], Loss: 2.2212\n",
      "Epoch [83/100], Step [100/500], Loss: 2.2112\n",
      "Epoch [83/100], Step [200/500], Loss: 2.2712\n",
      "Epoch [83/100], Step [300/500], Loss: 2.2012\n",
      "Epoch [83/100], Step [400/500], Loss: 2.1912\n",
      "Epoch [83/100], Step [500/500], Loss: 2.2412\n",
      "Epoch [84/100], Step [100/500], Loss: 2.2012\n",
      "Epoch [84/100], Step [200/500], Loss: 2.2012\n",
      "Epoch [84/100], Step [300/500], Loss: 2.2412\n",
      "Epoch [84/100], Step [400/500], Loss: 2.2312\n",
      "Epoch [84/100], Step [500/500], Loss: 2.2312\n",
      "Epoch [85/100], Step [100/500], Loss: 2.1712\n",
      "Epoch [85/100], Step [200/500], Loss: 2.2212\n",
      "Epoch [85/100], Step [300/500], Loss: 2.2390\n",
      "Epoch [85/100], Step [400/500], Loss: 2.2512\n",
      "Epoch [85/100], Step [500/500], Loss: 2.1412\n",
      "Epoch [86/100], Step [100/500], Loss: 2.2212\n",
      "Epoch [86/100], Step [200/500], Loss: 2.1512\n",
      "Epoch [86/100], Step [300/500], Loss: 2.2112\n",
      "Epoch [86/100], Step [400/500], Loss: 2.2415\n",
      "Epoch [86/100], Step [500/500], Loss: 2.2312\n",
      "Epoch [87/100], Step [100/500], Loss: 2.2412\n",
      "Epoch [87/100], Step [200/500], Loss: 2.2012\n",
      "Epoch [87/100], Step [300/500], Loss: 2.2412\n",
      "Epoch [87/100], Step [400/500], Loss: 2.2012\n",
      "Epoch [87/100], Step [500/500], Loss: 2.2112\n",
      "Epoch [88/100], Step [100/500], Loss: 2.1012\n",
      "Epoch [88/100], Step [200/500], Loss: 2.2312\n",
      "Epoch [88/100], Step [300/500], Loss: 2.1612\n",
      "Epoch [88/100], Step [400/500], Loss: 2.1712\n",
      "Epoch [88/100], Step [500/500], Loss: 2.1712\n",
      "Epoch [89/100], Step [100/500], Loss: 2.1812\n",
      "Epoch [89/100], Step [200/500], Loss: 2.1412\n",
      "Epoch [89/100], Step [300/500], Loss: 2.1312\n",
      "Epoch [89/100], Step [400/500], Loss: 2.1812\n",
      "Epoch [89/100], Step [500/500], Loss: 2.1812\n",
      "Epoch [90/100], Step [100/500], Loss: 2.2412\n",
      "Epoch [90/100], Step [200/500], Loss: 2.1311\n",
      "Epoch [90/100], Step [300/500], Loss: 2.2212\n",
      "Epoch [90/100], Step [400/500], Loss: 2.2512\n",
      "Epoch [90/100], Step [500/500], Loss: 2.1912\n",
      "Epoch [91/100], Step [100/500], Loss: 2.1712\n",
      "Epoch [91/100], Step [200/500], Loss: 2.1212\n",
      "Epoch [91/100], Step [300/500], Loss: 2.2312\n",
      "Epoch [91/100], Step [400/500], Loss: 2.1212\n",
      "Epoch [91/100], Step [500/500], Loss: 2.1712\n",
      "Epoch [92/100], Step [100/500], Loss: 2.1312\n",
      "Epoch [92/100], Step [200/500], Loss: 2.2412\n",
      "Epoch [92/100], Step [300/500], Loss: 2.1412\n",
      "Epoch [92/100], Step [400/500], Loss: 2.1512\n",
      "Epoch [92/100], Step [500/500], Loss: 2.1012\n",
      "Epoch [93/100], Step [100/500], Loss: 2.1612\n",
      "Epoch [93/100], Step [200/500], Loss: 2.1012\n",
      "Epoch [93/100], Step [300/500], Loss: 2.2011\n",
      "Epoch [93/100], Step [400/500], Loss: 2.1012\n",
      "Epoch [93/100], Step [500/500], Loss: 2.1512\n",
      "Epoch [94/100], Step [100/500], Loss: 2.1012\n",
      "Epoch [94/100], Step [200/500], Loss: 2.1712\n",
      "Epoch [94/100], Step [300/500], Loss: 2.1612\n",
      "Epoch [94/100], Step [400/500], Loss: 2.1112\n",
      "Epoch [94/100], Step [500/500], Loss: 2.2012\n",
      "Epoch [95/100], Step [100/500], Loss: 2.2012\n",
      "Epoch [95/100], Step [200/500], Loss: 2.1612\n",
      "Epoch [95/100], Step [300/500], Loss: 2.1311\n",
      "Epoch [95/100], Step [400/500], Loss: 2.2312\n",
      "Epoch [95/100], Step [500/500], Loss: 2.1412\n",
      "Epoch [96/100], Step [100/500], Loss: 2.1612\n",
      "Epoch [96/100], Step [200/500], Loss: 2.1312\n",
      "Epoch [96/100], Step [300/500], Loss: 2.1212\n",
      "Epoch [96/100], Step [400/500], Loss: 2.1712\n",
      "Epoch [96/100], Step [500/500], Loss: 2.0812\n",
      "Epoch [97/100], Step [100/500], Loss: 2.2312\n",
      "Epoch [97/100], Step [200/500], Loss: 2.1612\n",
      "Epoch [97/100], Step [300/500], Loss: 2.1612\n",
      "Epoch [97/100], Step [400/500], Loss: 2.2212\n",
      "Epoch [97/100], Step [500/500], Loss: 2.1612\n",
      "Epoch [98/100], Step [100/500], Loss: 2.2807\n",
      "Epoch [98/100], Step [200/500], Loss: 2.2212\n",
      "Epoch [98/100], Step [300/500], Loss: 2.0812\n",
      "Epoch [98/100], Step [400/500], Loss: 2.1612\n",
      "Epoch [98/100], Step [500/500], Loss: 2.0712\n",
      "Epoch [99/100], Step [100/500], Loss: 2.1612\n",
      "Epoch [99/100], Step [200/500], Loss: 2.1611\n",
      "Epoch [99/100], Step [300/500], Loss: 2.1112\n",
      "Epoch [99/100], Step [400/500], Loss: 2.1512\n",
      "Epoch [99/100], Step [500/500], Loss: 2.2012\n",
      "Epoch [100/100], Step [100/500], Loss: 2.1712\n",
      "Epoch [100/100], Step [200/500], Loss: 2.1312\n",
      "Epoch [100/100], Step [300/500], Loss: 2.1612\n",
      "Epoch [100/100], Step [400/500], Loss: 2.1712\n",
      "Epoch [100/100], Step [500/500], Loss: 2.2311\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images_np = np.asarray(images)\n",
    "        images = images.reshape(-1, 3, 32, 32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = my_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "3fs7RfqlPNQ-",
    "outputId": "7069da49-9d6f-4977-f888-3f2932e75ac8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 26.41 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model before performing adverserial attacks\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 3, 32, 32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = my_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsXwukTTPNRB"
   },
   "outputs": [],
   "source": [
    "for param in my_model.parameters():\n",
    "    param.requires_grad= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "oNisKtbMLeNd",
    "outputId": "3a3b946f-7365-4d1a-f6f6-6f1880c08d24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the network on the 10000 test images for epsilon value 0.0001:69.54 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0001:69.93 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0001:69.83 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0001:69.82 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0001:70.16 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0001:69.76 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0001:69.66 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0001:69.51 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0001:69.82 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0003:69.2 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0003:68.82 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0003:69.17 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0003:69.03 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0003:69.38 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0003:69.37 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0003:69.05 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0003:68.68 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.0003:69.46 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.001:67.21 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.001:66.59 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.001:67.34 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.001:67.68 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.001:67.48 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.001:67.92 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.001:67.29 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.001:66.33 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.001:67.49 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.003:65.54 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.003:64.36 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.003:65.67 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.003:65.73 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.003:65.73 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.003:65.66 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.003:65.29 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.003:63.51 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.003:65.3 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.01:63.32 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.01:62.25 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.01:63.41 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.01:63.63 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.01:63.66 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.01:64.0 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.01:63.62 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.01:61.93 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.01:63.39 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.03:59.19 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.03:57.6 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.03:58.74 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.03:58.66 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.03:58.9 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.03:59.62 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.03:59.46 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.03:57.06 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.03:58.91 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.1:47.81 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.1:45.74 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.1:46.61 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.1:48.01 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.1:46.93 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.1:48.38 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.1:48.6 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.1:45.66 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.1:48.29 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.3:29.89 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.3:27.39 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.3:28.34 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.3:29.69 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.3:30.3 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.3:30.2 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.3:30.69 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.3:28.57 %\n",
      "accuracy of the network on the 10000 test images for epsilon value 0.3:29.53 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Targeted FGSM attack\n",
    "epsilon=[0.0001,0.0003,0.001,0.003,0.01,0.03,0.1,0.3]\n",
    "average_accuracies = []\n",
    "save_images = []\n",
    "\n",
    "for value in epsilon:\n",
    "    accuracy = []\n",
    "    for k in range(1, 10):\n",
    "        l=[]\n",
    "        sign=[]\n",
    "        flag = 1\n",
    "        for i,(images,labels) in enumerate(test_loader):\n",
    "            copyOf_images=images.clone().detach()\n",
    "            copyOf_images=copyOf_images.reshape(-1,3,32,32).clone().to(device)\n",
    "            copyOf_images.requires_grad=True\n",
    "            target_class=(labels+k)%10  #missing classifying it with 10 classes though it has to be correct at least once\n",
    "            target_class=target_class.to(device)\n",
    "\n",
    "            #forward pass\n",
    "            outputs=my_model(copyOf_images)\n",
    "            loss=criterion(outputs,target_class)\n",
    "\n",
    "            #backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            c=copyOf_images.grad.sign()\n",
    "            #actual.append((images,labels))\n",
    "            copyOf_images=copyOf_images-(value*c) \n",
    "            copyOf_images=torch.clamp(copyOf_images,0,1)\n",
    "            sign.append(c)\n",
    "            l.append((copyOf_images,labels))\n",
    "            if(k == 2 and flag == 1):\n",
    "              save_images.append(copyOf_images[0].cpu().detach().permute(1, 2, 0))\n",
    "              flag = 0\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            correct=0\n",
    "            total=0\n",
    "            for images,labels in l:\n",
    "                images=images.reshape(-1,3,32,32).to(device)\n",
    "                labels=labels.to(device)\n",
    "                outputs=my_model(images)\n",
    "                _,predicted=torch.max(outputs.data,1)\n",
    "                total+=labels.size(0)\n",
    "                correct+=(predicted==labels).sum().item()\n",
    "                accuracy.append(100*correct/total)\n",
    "            print(\"accuracy of the network on the 10000 test images for epsilon value {}:{} %\".format(value,100*correct/total)) \n",
    "    accuracy = np.asarray(accuracy)\n",
    "    average_accuracies.append(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2WTig7oHbb2i",
    "outputId": "5b65c366-1afe-44c9-e4e0-78c7f1eb487d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69.9217480208212, 69.27044238765257, 67.26257596691657, 65.60755103637499, 63.32840495638567, 59.063238195821405, 47.574498151106354, 29.550498492383994]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "ocBPg5GNQtOh",
    "outputId": "af02cc64-5c2b-4a04-c560-ac26a1f87245"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2de1e530b8>"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(copyOf_images[0].cpu().detach().permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "p61gqyX_PNRE",
    "outputId": "c50410fd-d0c3-4d93-af19-325fb52a93d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "sign = []\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(test_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 3, 32, 32).to(device)\n",
    "        images.requires_grad=True\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = my_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "\n",
    "        loss.backward()\n",
    "        c =images.grad.sign()\n",
    "        images = images+ 0.01* c\n",
    "        sign.append(c)\n",
    "        l.append((images,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "taLm4VrNPNRH"
   },
   "outputs": [],
   "source": [
    "im, _ = l[0]\n",
    "im = im.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iTY_3WWPNRK"
   },
   "outputs": [],
   "source": [
    "im = im.reshape(100, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37muVykKPNRN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "z3BLTBCXPNRQ",
    "outputId": "9ecef8c3-f6d6-4c01-d05f-d95157096792"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4c7e1ccef0>"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRKYo+IXIlP8H/P4H2TrHJUUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(im[2, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "iDKMQNCWPNRT",
    "outputId": "c9a3fff8-b779-45d8-98d3-beb7ea75b3e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 48.88 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in l:\n",
    "        images = images.reshape(-1, 3, 32, 32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = my_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "iv6zlWjxPNRW",
    "outputId": "ea57a2eb-75c0-4b77-a333-18b191734939"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "epsilon=[0.0001,0.0003,0.001,0.003,0.01,0.03,0.1,0.3]\n",
    "epsilon = np.asarray(epsilon)\n",
    "epsilon = np.log10(epsilon)\n",
    "accuracy = [69.92, 69.27, 67.26, 65.60, 63.32, 59.06, 47.57, 29.55]\n",
    "\n",
    "plt.plot(epsilon, accuracy)\n",
    "plt.xlabel(\"Epsilon on log scale\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Accuracy for Targeted FGSM attack on CIFAR10 Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1q_sZIUaX50Q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FGSM_CIFAR.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
