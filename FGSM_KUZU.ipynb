{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST dataset \ntrain_dataset = torchvision.datasets.KMNIST(root='../../data',        #instead of KMNIST , just type CIFAR10 . Please check the spelling\n                                           train=True, \n                                           transform=transforms.ToTensor(),  \n                                           download=True)\n\ntest_dataset = torchvision.datasets.KMNIST(root='../../data', \n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Fully connected neural network with one hidden layer\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\nmodel = NeuralNet(input_size, hidden_size, num_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n\n# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Move tensors to the configured device\n        images = images.reshape(-1, 28*28).to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 300 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n# Test the model\n# In test phase, we don't need to compute gradients (for memory efficiency)","execution_count":99,"outputs":[{"output_type":"stream","text":"\r0it [00:00, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz to ../../data/KMNIST/raw/train-images-idx3-ubyte.gz\n","name":"stdout"},{"output_type":"stream","text":"18169856it [00:02, 6279032.09it/s]                              \n","name":"stderr"},{"output_type":"stream","text":"Extracting ../../data/KMNIST/raw/train-images-idx3-ubyte.gz\n","name":"stdout"},{"output_type":"stream","text":"\r0it [00:00, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz to ../../data/KMNIST/raw/train-labels-idx1-ubyte.gz\n","name":"stdout"},{"output_type":"stream","text":"32768it [00:00, 59180.75it/s]                            \n0it [00:00, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Extracting ../../data/KMNIST/raw/train-labels-idx1-ubyte.gz\nDownloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz to ../../data/KMNIST/raw/t10k-images-idx3-ubyte.gz\n","name":"stdout"},{"output_type":"stream","text":"3047424it [00:01, 1775629.98it/s]                             \n0it [00:00, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Extracting ../../data/KMNIST/raw/t10k-images-idx3-ubyte.gz\nDownloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz to ../../data/KMNIST/raw/t10k-labels-idx1-ubyte.gz\n","name":"stdout"},{"output_type":"stream","text":"8192it [00:00, 19691.04it/s]            \n","name":"stderr"},{"output_type":"stream","text":"Extracting ../../data/KMNIST/raw/t10k-labels-idx1-ubyte.gz\nProcessing...\nDone!\nEpoch [1/5], Step [300/600], Loss: 0.4676\nEpoch [1/5], Step [600/600], Loss: 0.1444\nEpoch [2/5], Step [300/600], Loss: 0.1817\nEpoch [2/5], Step [600/600], Loss: 0.2526\nEpoch [3/5], Step [300/600], Loss: 0.1385\nEpoch [3/5], Step [600/600], Loss: 0.1645\nEpoch [4/5], Step [300/600], Loss: 0.0762\nEpoch [4/5], Step [600/600], Loss: 0.1076\nEpoch [5/5], Step [300/600], Loss: 0.0426\nEpoch [5/5], Step [600/600], Loss: 0.0360\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nclass Kuzushiji(nn.Module):\n    def __init__(self):\n        super(Kuzushiji, self).__init__()\n        self.pad1 = nn.ZeroPad2d((2, 2))\n        \n        self.layer1 = nn.Sequential(\n        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(7, 7), stride=(1, 1), bias=False), \n        nn.BatchNorm2d(num_features=32),\n        nn.MaxPool2d(kernel_size=(2,2)),\n        nn.ReLU()\n        )\n    \n        \n        self.layer2 = nn.Sequential(\n        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=(1, 1), bias=False), \n        nn.BatchNorm2d(num_features=64),\n        nn.ReLU()\n        )\n        \n        self.layer3 = nn.Sequential(\n        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), bias=False), \n        nn.BatchNorm2d(num_features=128),\n        nn.MaxPool2d(kernel_size=(2,2)),\n        nn.ReLU()\n        )\n        \n        self.fc1 = nn.Sequential(\n        nn.Linear(in_features=768, out_features=512),\n        nn.ReLU()\n        )\n        self.dropout1 = nn.Dropout(p=0.2)\n        \n        self.fc2 = nn.Sequential(\n        nn.Linear(in_features=512, out_features=10),\n        nn.Softmax()\n        )\n    def forward(self, x):\n        x = self.pad1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = x.reshape(x.shape[0], -1)\n       # print(x.shape)\n        x = self.fc1(x)\n        x = self.dropout1(x)\n        out = self.fc2(x)\n        \n        return out","execution_count":127,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model = Kuzushiji().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate) ","execution_count":128,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Move tensors to the configured device\n        images = images.reshape(-1, 1, 28,28).to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = my_model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))","execution_count":129,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  input = module(input)\n","name":"stderr"},{"output_type":"stream","text":"Epoch [1/5], Step [100/600], Loss: 1.5820\nEpoch [1/5], Step [200/600], Loss: 1.5523\nEpoch [1/5], Step [300/600], Loss: 1.5631\nEpoch [1/5], Step [400/600], Loss: 1.5315\nEpoch [1/5], Step [500/600], Loss: 1.5182\nEpoch [1/5], Step [600/600], Loss: 1.5393\nEpoch [2/5], Step [100/600], Loss: 1.4870\nEpoch [2/5], Step [200/600], Loss: 1.5156\nEpoch [2/5], Step [300/600], Loss: 1.4956\nEpoch [2/5], Step [400/600], Loss: 1.5423\nEpoch [2/5], Step [500/600], Loss: 1.4985\nEpoch [2/5], Step [600/600], Loss: 1.5378\nEpoch [3/5], Step [100/600], Loss: 1.5008\nEpoch [3/5], Step [200/600], Loss: 1.5116\nEpoch [3/5], Step [300/600], Loss: 1.4695\nEpoch [3/5], Step [400/600], Loss: 1.5066\nEpoch [3/5], Step [500/600], Loss: 1.5287\nEpoch [3/5], Step [600/600], Loss: 1.5420\nEpoch [4/5], Step [100/600], Loss: 1.5008\nEpoch [4/5], Step [200/600], Loss: 1.4962\nEpoch [4/5], Step [300/600], Loss: 1.5310\nEpoch [4/5], Step [400/600], Loss: 1.5053\nEpoch [4/5], Step [500/600], Loss: 1.4961\nEpoch [4/5], Step [600/600], Loss: 1.4847\nEpoch [5/5], Step [100/600], Loss: 1.5314\nEpoch [5/5], Step [200/600], Loss: 1.5337\nEpoch [5/5], Step [300/600], Loss: 1.5021\nEpoch [5/5], Step [400/600], Loss: 1.4905\nEpoch [5/5], Step [500/600], Loss: 1.4962\nEpoch [5/5], Step [600/600], Loss: 1.5121\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.reshape(-1, 1, 28, 28).to(device)\n        labels = labels.to(device)\n        outputs = my_model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))","execution_count":131,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  input = module(input)\n","name":"stderr"},{"output_type":"stream","text":"Accuracy of the network on the 10000 test images: 91.64 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in my_model.parameters():\n    param.requires_grad= False","execution_count":132,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l =[]\nsign = []\nfor epoch in range(1):\n    for i, (images, labels) in enumerate(test_loader):  \n        # Move tensors to the configured device\n        images = images.reshape(-1, 1,28,28).to(device)\n        images.requires_grad=True\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = my_model(images)\n        loss = criterion(outputs, labels)\n\n#         # Backward and optimize\n\n        loss.backward()\n        c =images.grad.sign()\n        images = images+ 0.01* c\n        sign.append(c)\n        l.append((images,labels))\n\n","execution_count":141,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  input = module(input)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"im, _ = l[0]\nim = im.data.cpu().numpy()","execution_count":142,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im = im.reshape(100, 28, 28)","execution_count":143,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n","execution_count":144,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(im[2])","execution_count":145,"outputs":[{"output_type":"execute_result","execution_count":145,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7fa78a7962b0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in l:\n        images = images.reshape(-1,1, 28,28).to(device)\n        labels = labels.to(device)\n        outputs = my_model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))","execution_count":146,"outputs":[{"output_type":"stream","text":"Accuracy of the network on the 10000 test images: 88.62 %\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  input = module(input)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}