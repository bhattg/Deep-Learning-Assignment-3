{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 20\nbatch_size = 128\nlearning_rate = 0.005\n\n# MNIST dataset \ntrain_dataset = torchvision.datasets.KMNIST(root='../../data',        #instead of KMNIST , just type CIFAR10 . Please check the spelling\n                                           train=True, \n                                           transform=transforms.ToTensor(),  \n                                           download=True)\n\ntest_dataset = torchvision.datasets.KMNIST(root='../../data', \n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Fully connected neural network with one hidden layer\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\nmodel = NeuralNet(input_size, hidden_size, num_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n\n# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Move tensors to the configured device\n        images = images.reshape(-1, 28*28).to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 300 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n# # Test the model\n# # In test phase, we don't need to compute gradients (for memory efficiency)","execution_count":25,"outputs":[{"output_type":"stream","text":"Epoch [1/20], Step [300/469], Loss: 0.1445\nEpoch [2/20], Step [300/469], Loss: 0.0911\nEpoch [3/20], Step [300/469], Loss: 0.0828\nEpoch [4/20], Step [300/469], Loss: 0.0885\nEpoch [5/20], Step [300/469], Loss: 0.0326\nEpoch [6/20], Step [300/469], Loss: 0.0721\nEpoch [7/20], Step [300/469], Loss: 0.0343\nEpoch [8/20], Step [300/469], Loss: 0.0304\nEpoch [9/20], Step [300/469], Loss: 0.0385\nEpoch [10/20], Step [300/469], Loss: 0.0084\nEpoch [11/20], Step [300/469], Loss: 0.0369\nEpoch [12/20], Step [300/469], Loss: 0.0162\nEpoch [13/20], Step [300/469], Loss: 0.0312\nEpoch [14/20], Step [300/469], Loss: 0.0553\nEpoch [15/20], Step [300/469], Loss: 0.0244\nEpoch [16/20], Step [300/469], Loss: 0.0316\nEpoch [17/20], Step [300/469], Loss: 0.0045\nEpoch [18/20], Step [300/469], Loss: 0.0060\nEpoch [19/20], Step [300/469], Loss: 0.1502\nEpoch [20/20], Step [300/469], Loss: 0.1290\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing=[]\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        testing.append((images, labels))\n        images = images.reshape(-1, 28*28).to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test accuracy: {} %'.format(100 * correct / total))\n    correct = 0\n    total = 0\n    for images, labels in train_loader:\n        testing.append((images, labels))\n        images = images.reshape(-1, 28*28).to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Train accuracy: {} %'.format(100 * correct / total))","execution_count":26,"outputs":[{"output_type":"stream","text":"Test accuracy: 90.97 %\nTrain accuracy: 99.45833333333333 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad= False","execution_count":27,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now generating adverserial examples for the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nl =[]\nsign = []\nfor epoch in range(1):\n    for i, (images, labels) in enumerate(train_loader):  \n# Move tensors to the configured device\n        images = images.reshape(-1,28*28).to(device)\n        images.requires_grad=True\n        labels = labels.to(device)\n        # Forward pass\n        outputs = model(images)\n   #     print(labels.shape)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        c =images.grad.sign()\n        images = images+ 0.01* c\n        sign.append(c)\n        l.append((images,labels))\n\n","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nl_test =[]\nsign_test = []\nfor epoch in range(1):\n    for i, (images, labels) in enumerate(test_loader):  \n# Move tensors to the configured device\n        images = images.reshape(-1,28*28).to(device)\n        images.requires_grad=True\n        labels = labels.to(device)\n        # Forward pass\n        outputs = model(images)\n   #     print(labels.shape)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        c =images.grad.sign()\n        images = images+ 0.01* c\n        sign_test.append(c)\n        l_test.append((images,labels))\n\n","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in l:\n        images = images.reshape(-1,28*28).to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n   #     print(predicted)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy on adverserial images of training dataa: {} %'.format(100 * correct / total))","execution_count":29,"outputs":[{"output_type":"stream","text":"Accuracy on adverserial images of training dataa: 96.34 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in l_test:\n        images = images.reshape(-1,28*28).to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n   #     print(predicted)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy on adverserial images of testing data: {} %'.format(100 * correct / total))","execution_count":31,"outputs":[{"output_type":"stream","text":"Accuracy on adverserial images of testing data: 82.97 %\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Now doing adverserial training**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_new = NeuralNet(input_size, hidden_size, num_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model_new.parameters(), lr=learning_rate)  ","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tot = total_step\ntotal_step = 2*len(train_loader)\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Move tensors to the configured device\n        images = images.reshape(-1, 28*28).to(device)\n        labels = labels.to(device)\n    \n        # Forward pass\n        outputs = model_new(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if (i+1) % 300 == 0:\n                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n    for (images, labels) in l:\n        i+=1\n        images = images.reshape(-1, 28*28).to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model_new(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n     #   print(i)\n        if (i+1) % 300 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))","execution_count":33,"outputs":[{"output_type":"stream","text":"Epoch [1/20], Step [300/938], Loss: 0.2502\nEpoch [1/20], Step [600/938], Loss: 0.0726\nEpoch [1/20], Step [900/938], Loss: 0.3586\nEpoch [2/20], Step [300/938], Loss: 0.0943\nEpoch [2/20], Step [600/938], Loss: 0.0251\nEpoch [2/20], Step [900/938], Loss: 0.2766\nEpoch [3/20], Step [300/938], Loss: 0.0136\nEpoch [3/20], Step [600/938], Loss: 0.0488\nEpoch [3/20], Step [900/938], Loss: 0.1411\nEpoch [4/20], Step [300/938], Loss: 0.0671\nEpoch [4/20], Step [600/938], Loss: 0.0870\nEpoch [4/20], Step [900/938], Loss: 0.0754\nEpoch [5/20], Step [300/938], Loss: 0.0136\nEpoch [5/20], Step [600/938], Loss: 0.0166\nEpoch [5/20], Step [900/938], Loss: 0.0158\nEpoch [6/20], Step [300/938], Loss: 0.0009\nEpoch [6/20], Step [600/938], Loss: 0.0082\nEpoch [6/20], Step [900/938], Loss: 0.0142\nEpoch [7/20], Step [300/938], Loss: 0.0725\nEpoch [7/20], Step [600/938], Loss: 0.0086\nEpoch [7/20], Step [900/938], Loss: 0.0835\nEpoch [8/20], Step [300/938], Loss: 0.0401\nEpoch [8/20], Step [600/938], Loss: 0.0317\nEpoch [8/20], Step [900/938], Loss: 0.1034\nEpoch [9/20], Step [300/938], Loss: 0.2018\nEpoch [9/20], Step [600/938], Loss: 0.0002\nEpoch [9/20], Step [900/938], Loss: 0.2020\nEpoch [10/20], Step [300/938], Loss: 0.0004\nEpoch [10/20], Step [600/938], Loss: 0.0000\nEpoch [10/20], Step [900/938], Loss: 0.0322\nEpoch [11/20], Step [300/938], Loss: 0.0725\nEpoch [11/20], Step [600/938], Loss: 0.0002\nEpoch [11/20], Step [900/938], Loss: 0.1137\nEpoch [12/20], Step [300/938], Loss: 0.0141\nEpoch [12/20], Step [600/938], Loss: 0.0010\nEpoch [12/20], Step [900/938], Loss: 0.0202\nEpoch [13/20], Step [300/938], Loss: 0.0629\nEpoch [13/20], Step [600/938], Loss: 0.0001\nEpoch [13/20], Step [900/938], Loss: 0.0010\nEpoch [14/20], Step [300/938], Loss: 0.0120\nEpoch [14/20], Step [600/938], Loss: 0.0010\nEpoch [14/20], Step [900/938], Loss: 0.0438\nEpoch [15/20], Step [300/938], Loss: 0.0644\nEpoch [15/20], Step [600/938], Loss: 0.0175\nEpoch [15/20], Step [900/938], Loss: 0.1850\nEpoch [16/20], Step [300/938], Loss: 0.0000\nEpoch [16/20], Step [600/938], Loss: 0.0011\nEpoch [16/20], Step [900/938], Loss: 0.0897\nEpoch [17/20], Step [300/938], Loss: 0.0250\nEpoch [17/20], Step [600/938], Loss: 0.0054\nEpoch [17/20], Step [900/938], Loss: 0.0001\nEpoch [18/20], Step [300/938], Loss: 0.0000\nEpoch [18/20], Step [600/938], Loss: 0.0000\nEpoch [18/20], Step [900/938], Loss: 0.0001\nEpoch [19/20], Step [300/938], Loss: 0.0000\nEpoch [19/20], Step [600/938], Loss: 0.0226\nEpoch [19/20], Step [900/938], Loss: 0.0157\nEpoch [20/20], Step [300/938], Loss: 0.0004\nEpoch [20/20], Step [600/938], Loss: 0.0000\nEpoch [20/20], Step [900/938], Loss: 0.1791\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in l_test:\n        images = images.reshape(-1,28*28).to(device)\n        labels = labels.to(device)\n        outputs = model_new(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy on adverserial images of test data: {} %'.format(100 * correct / total))","execution_count":35,"outputs":[{"output_type":"stream","text":"Accuracy on adverserial images of test data: 89.37 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in l:\n        images = images.reshape(-1,28*28).to(device)\n        labels = labels.to(device)\n        outputs = model_new(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy on adverserial images of train data: {} %'.format(100 * correct / total))","execution_count":36,"outputs":[{"output_type":"stream","text":"Accuracy on adverserial images of train data: 99.55 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}