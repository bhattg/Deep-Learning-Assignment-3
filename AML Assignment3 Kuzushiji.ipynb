{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport os\nfrom tqdm import tqdm\nfrom pylab import *","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 15\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST dataset \ntrain_dataset = torchvision.datasets.KMNIST(root='../../data',      \n                                           train=True, \n                                           transform=transforms.ToTensor(),\n                                           download=True)\n\ntest_dataset = torchvision.datasets.KMNIST(root='../../data', \n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self,input_size,hidden_size,num_classes):\n        super(NeuralNet,self).__init__()\n        self.fc1=nn.Linear(input_size,hidden_size)\n        self.relu=nn.ReLU()\n        self.fc2=nn.Linear(hidden_size,num_classes)\n        \n    def forward(self,x):\n        out=self.fc1(x)\n        out=self.relu(out)\n        out=self.fc2(out)\n        return out\n    \nmodel=NeuralNet(input_size,hidden_size,num_classes).to(device)\n\n#loss and optimiser\ncriterion=nn.CrossEntropyLoss()\noptimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass Kuzushiji(nn.Module):\n    def __init__(self):\n        super(Kuzushiji, self).__init__()\n        self.pad1 = nn.ZeroPad2d((2, 2))\n        \n        self.layer1 = nn.Sequential(\n        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(7, 7), stride=(1, 1), bias=False), \n        nn.BatchNorm2d(num_features=32),\n        nn.MaxPool2d(kernel_size=(2,2)),\n        nn.ReLU()\n        )\n    \n        \n        self.layer2 = nn.Sequential(\n        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=(1, 1), bias=False), \n        nn.BatchNorm2d(num_features=64),\n        nn.ReLU()\n        )\n        \n        self.layer3 = nn.Sequential(\n        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), bias=False), \n        nn.BatchNorm2d(num_features=128),\n        nn.MaxPool2d(kernel_size=(2,2)),\n        nn.ReLU()\n        )\n        \n        self.fc1 = nn.Sequential(\n        nn.Linear(in_features=768, out_features=512),\n        nn.ReLU()\n        )\n        self.dropout1 = nn.Dropout(p=0.2)\n        \n        self.fc2 = nn.Sequential(\n        nn.Linear(in_features=512, out_features=10),\n        nn.Softmax()\n        )\n        \n    def forward(self, x):\n        x = self.pad1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = x.reshape(x.shape[0], -1)\n       # print(x.shape)\n        x = self.fc1(x)\n        x = self.dropout1(x)\n        out = self.fc2(x)\n        \n        return out,x","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model=Kuzushiji().to(device)\ncriterion=nn.CrossEntropyLoss()\noptimizer=torch.optim.Adam(my_model.parameters(),lr=learning_rate)","execution_count":63,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-722e19c553cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKuzushiji\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Move tensors to the configured device\n        images = images.reshape(-1, 1, 28,28).to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = my_model(images)[0]\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normal Model Testing\nwith torch.no_grad():\n    correct=0\n    total=0\n    for images,labels in test_loader:\n        images=images.reshape(-1,1,28,28).to(device)\n        labels=labels.to(device)\n        outputs=my_model(images)[0]\n        _,predicted=torch.max(outputs.data,1)\n        total+=labels.size(0)\n        correct+=(predicted==labels).sum().item()\n        \n    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FGSM Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in my_model.parameters():\n    param.requires_grad=False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Targeted FGSM attack "},{"metadata":{"trusted":true},"cell_type":"code","source":"#targeted fgsm \n\n#epsilon=[0.0001,0.0003,0.001,0.003,0.1,0.2,0.3,0.4,0.5]\npred_list=[]\nepsilon=[0.3]\n\nfor value in epsilon:\n    pred_val_list=[]\n    for k in range(2,3):\n        l_tfgsm=[]\n        sign=[]\n        #actual=[]\n        for i,(images,labels) in enumerate(test_loader):\n            copyOf_images=images.clone().detach()\n            copyOf_images=copyOf_images.reshape(-1,1,28,28).to(device)\n            copyOf_images.requires_grad=True\n            target_class=(labels+k)%10  #missing classifying it with 10 classes though it has to be correct at least once\n            target_class=target_class.to(device)\n\n            #forward pass\n            outputs=my_model(copyOf_images)[0]\n            loss=criterion(outputs,target_class)\n\n            #backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            c=copyOf_images.grad.sign()\n            #actual.append((images,labels))\n            copyOf_images=copyOf_images-(value*c) \n            copyOf_images=torch.clamp(copyOf_images,0,1)\n            sign.append(c)\n            l_tfgsm.append((copyOf_images,images,labels))\n\n        with torch.no_grad():\n            correct=0\n            total=0\n            for images,trueimages,labels in l_tfgsm:\n                images=images.reshape(-1,1,28,28).to(device)\n                labels=labels.to(device)\n                outputs=my_model(images)[0]\n                _,predicted=torch.max(outputs.data,1)\n                total+=labels.size(0)\n                correct+=(predicted==labels).sum().item()\n            pred_val_list.append(100*correct/total)\n            print(\"accuracy of the network on the 10000 test images for epsilon value {}:{} %\".format(value,100*correct/total)) \n    pred_list.append(pred_val_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pred_list)\n\nimport matplotlib.pyplot as plt\n\nmake_list=[]\nepsilon=[0.0001,0.0003,0.001,0.003,0.1,0.2,0.3,0.4,0.5]\n\nfor k in pred_list:\n    make_list.append(k[2])\nplt.plot(epsilon,make_list)\nplt.xlabel(\"Epsilon Value\")\nplt.ylabel(\"Accuracy(in %)\")\nplt.title(\"Targeted FGSM on Kuzushiji where Target labels =(True label+2)%10\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Untargeted FGSM attack \nl=[]\nsign=[]\nactual=[]\nepsilon=[0.0001,0.0003,0.001,0.003,0.1,0.2,0.3,0.4,0.5]\n\nfor value in epsilon:\n   # for k in range(10):\n        l=[]\n        sign=[]\n        #actual=[]\n        for i,(images,labels) in enumerate(test_loader):\n            copyOf_images=images.clone().detach()\n            copyOf_images=copyOf_images.reshape(-1,1,28,28).to(device)\n            copyOf_images.requires_grad=True\n            #target_class=(labels+k)%10  #missing classifying it with 10 classes though it has to be correct at least once\n            target_class=labels\n            target_class=target_class.to(device)\n            \n            #forward pass\n            outputs=my_model(copyOf_images)\n            loss=criterion(outputs,target_class)\n\n            #backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            c=copyOf_images.grad.sign()\n            #actual.append((images,labels))\n            copyOf_images=copyOf_images+(value*c) \n            copyOf_images=torch.clamp(copyOf_images,0,1)\n            sign.append(c)\n            l.append((copyOf_images,labels))\n\n        with torch.no_grad():\n            correct=0\n            total=0\n            for images,labels in l:\n                images=images.reshape(-1,1,28,28).to(device)\n                labels=labels.to(device)\n                outputs=my_model(images)\n                _,predicted=torch.max(outputs.data,1)\n                total+=labels.size(0)\n                correct+=(predicted==labels).sum().item()\n            print(\"accuracy of the network on the 10000 test images for epsilon value {}:{} %\".format(value,100*correct/total)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the accuracy after FGSM attack\n\nwith torch.no_grad():\n    correct=0\n    total=0\n    for images,labels in l:\n        images=images.reshape(-1,1,28,28).to(device)\n        labels=labels.to(device)\n        outputs=my_model(images)\n        _,predicted=torch.max(outputs.data,1)\n        total+=labels.size(0)\n        correct+=(predicted==labels).sum().item()\n    print(\"accuracy of the network on the 10000 test images:{} %\".format(100*correct/total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_var(x, requires_grad=False, volatile=False):\n    \"\"\"\n    Varialbe type that automatically choose cpu or cuda\n    \"\"\"\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable(x, requires_grad=requires_grad, volatile=volatile)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PGD Attack \n\nclass LinfPGDAttack(nn.Module):\n    \n    def __init__(self,model=None,epsilon=0.3,k=40,a=0.01,random_start=True):\n        super(LinfPGDAttack, self).__init__()\n        self.model=model\n        self.epsilon=epsilon\n        self.k=k\n        self.a=a\n        self.randn=random_start\n        self.loss_fn=nn.CrossEntropyLoss() \n    \n    def perturb(self,X_nat,y):\n        \n        if self.randn:\n            X=X_nat+np.random.uniform(-self.epsilon,self.epsilon,X_nat.shape).astype('float32')\n        else:\n            X=np.copy(X_nat)\n        \n        for i in range(self.k):\n            X_var=to_var(torch.from_numpy(X),requires_grad=True)\n            y_var=to_var(torch.LongTensor(y))\n            \n            scores=self.model(X_var)[0]\n            loss=self.loss_fn(scores,y_var)\n            loss.backward()\n            grad=X_var.grad.data.cpu().numpy()\n            \n            X-=self.a*np.sign(grad)\n            \n            X=np.clip(X,X_nat-self.epsilon,X_nat+self.epsilon)\n            X=np.clip(X,0,1)\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.autograd import Variable\n\nl=[]\n#perturbed_images=[]\nepsilon=[0.001,0.003,0.01,0.03,0.1,0.2,0.3,0.4,0.5]\ntmp_list=[]\n\nfor value in epsilon:\n    for i,(images,labels) in enumerate(test_loader):       \n        PGDobj=LinfPGDAttack(model=my_model,random_start=False,epsilon=value)\n        target=(labels+1)%10  #generating the target\n        Changed_Images=PGDobj.perturb(images.numpy(),target.numpy())\n        if np.array_equal(Changed_Images,images.numpy()):\n            print(\"No change\")\n            break        \n        l.append((torch.from_numpy(Changed_Images),labels))\n        if(i%50==0):\n            print(\"Now adding changed images in {}th iteration\".format(i))\n\n    with torch.no_grad():\n        correct=0\n        total=0\n        for images,labels in l:\n            images=images.reshape(-1,1,28,28).to(device)\n            labels=labels.to(device)\n            outputs=my_model(images)[0]\n            _,predicted=torch.max(outputs.data,1)\n            total+=labels.size(0)\n            correct+=(predicted==labels).sum().item()\n        tmp_list.append(100*correct/total)\n        print(\"accuracy of the network on the 10000 test images:{} %\".format(100*correct/total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.autograd import Variable\n\nl=[]\n#perturbed_images=[]\na=[0.001,0.003,0.01,0.03,0.1,0.2,0.3,0.4,0.5]\ntmp_list=[]\n\nfor value in epsilon:\n    for i,(images,labels) in enumerate(test_loader):       \n        PGDobj=LinfPGDAttack(model=my_model,random_start=False,a=value)\n        target=(labels+1)%10  #generating the target\n        Changed_Images=PGDobj.perturb(images.numpy(),target.numpy())\n        if np.array_equal(Changed_Images,images.numpy()):\n            print(\"No change\")\n            break        \n        l.append((torch.from_numpy(Changed_Images),labels))\n        if(i%50==0):\n            print(\"Now adding changed images in {}th iteration\".format(i))\n\n    with torch.no_grad():\n        correct=0\n        total=0\n        for images,labels in l:\n            images=images.reshape(-1,1,28,28).to(device)\n            labels=labels.to(device)\n            outputs=my_model(images)\n            _,predicted=torch.max(outputs.data,1)\n            total+=labels.size(0)\n            correct+=(predicted==labels).sum().item()\n        tmp_list.append(100*correct/total)\n        print(\"accuracy of the network on the 10000 test images:{} %\".format(100*correct/total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(a,tmp_list)\nplt.title(\"Targeted PGD over Kuzushiji\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Value of One Step of Gradient Descent\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the accuracy after FGSM attack\nwith torch.no_grad():\n    correct=0\n    total=0\n    for images,labels in l:\n        images=images.reshape(-1,1,28,28).to(device)\n        labels=labels.to(device)\n        outputs=my_model(images)\n        _,predicted=torch.max(outputs.data,1)\n        total+=labels.size(0)\n        correct+=(predicted==labels).sum().item()\n    \n    print(\"accuracy of the network on the 10000 test images:{} %\".format(100*correct/total))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Carlini Wagner Attack"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CW-L2 Attack\n# Based on the paper, i.e. not exact same version of the code on https://github.com/carlini/nn_robust_attacks\n# (1) Binary search method for c, (2) Optimization on tanh space, (3) Choosing method best l2 adversaries is NOT IN THIS CODE.\ndef cw_l2_attack(model, images, labels, targeted=False, c=1e-4, kappa=0, max_iter=200, learning_rate=0.01) :\n\n    images = images.to(device)     \n    labels = labels.to(device)\n\n    # Define f-function\n    def f(x) :\n\n        outputs = model(x)[1]  #taking penultimate layer in this case rather than the softmax layer\n        one_hot_labels = torch.eye(len(outputs[0]))[labels].to(device)\n\n        i, _ = torch.max((1-one_hot_labels)*outputs, dim=1)\n        j = torch.masked_select(outputs, one_hot_labels.byte())\n        \n        # If targeted, optimize for making the other class most likely \n        if targeted :\n            return torch.clamp(i-j, min=-kappa)\n        \n        # If untargeted, optimize for making the other class most likely \n        else :\n            return torch.clamp(j-i, min=-kappa)\n    \n    w = torch.zeros_like(images, requires_grad=True).to(device)\n\n    optimizer = torch.optim.Adam([w], lr=learning_rate)\n\n    prev = 1e10\n    \n    for step in tqdm(range(max_iter)) :\n\n        a = 1/2*(nn.Tanh()(w) + 1)\n\n        loss1 = nn.MSELoss(reduction='sum')(a, images)\n        loss2 = torch.sum(c*f(a))\n\n        cost = loss1 + loss2\n\n        optimizer.zero_grad()\n        cost.backward()\n        optimizer.step()\n\n        # Early Stop when loss does not converge.\n        if step % (max_iter//10) == 0 :\n            if cost > prev :\n                print('Attack Stopped due to CONVERGENCE....')\n                return a\n            prev = cost\n        \n        print('- Learning Progress : %2.2f %%        ' %((step+1)/max_iter*100), end='\\r')\n\n    attack_images = 1/2*(nn.Tanh()(w) + 1)\n\n    return attack_images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Attack Image & Predicted Label\")\n\n#kappa=[0.01,0.03,1,3,5,7,9]\n#options=[0.01,0.03,0.1,0.2,0.3,0.4,0.5]\noptions=[0.5]\ntmp_list=[]\nl=[]\nfor value in options:\n    correct = 0\n    total = 0\n    for i,(images, labels) in enumerate(test_loader):\n        if (i<20):\n            #target=(labels+2)%10 \n            images_ = cw_l2_attack(my_model, images, labels, targeted=True,c=value,kappa=5)\n            #images = cw_l2_attack(my_model, images, target, c=0.1)\n            labels = labels.to(device)\n            outputs = my_model(images_)[0]\n            \n            _, pre = torch.max(outputs.data, 1)\n            l.append((images_,images,labels))\n            total+=labels.size(0)\n            correct += (pre == labels).sum().item()\n\n            #imshow(torchvision.utils.make_grid(images.cpu().data, normalize=True), [normal_data.classes[i] for i in pre])\n    tmp_list.append(100*float(correct)/total)\n    print('Accuracy of test text: %f %%' % (100 * float(correct) / total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.asarray(l).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tmp_list)\n\nimport matplotlib.pyplot as plt\nplt.title(\"Targeted L2 Carlini Wagner Attack on Kuzushiji\")\nplt.xlabel(\"Different k values keeping c=0.3,learning_rate=0.01,num_iters=200\")\nplt.ylabel(\"Accuracy\")\nplt.plot(kappa_list,tmp_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_list\n\nimport matplotlib.pyplot as plt\nplt.title(\"Targeted L2 Carlini Wagner Attack on Kuzushiji\")\nplt.xlabel(\"Different c values keeping k=5,learning_rate=0.01,num_iters=200\")\nplt.ylabel(\"Accuracy\")\nplt.plot(options,tmp_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.utils import save_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create a directory if not exists\nsample_dir = 'samples'\nif not os.path.exists(sample_dir):\n    os.makedirs(sample_dir)\n\n# Hyper-parameters\nimage_size = 784\nh_dim = 400\nz_dim = 60\nnum_epochs = 20\nbatch_size = 64\nlearning_rate = 2e-3\n\n# MNIST dataset\ndataset = torchvision.datasets.KMNIST(root='../../data',\n                                     train=True,\n                                     transform=transforms.ToTensor(),\n                                     download=True)\n\n# Data loader\ndata_loader = torch.utils.data.DataLoader(dataset=dataset,\n                                          batch_size=batch_size, \n                                          shuffle=True)\n\n\n# VAE model\nclass VAE(nn.Module):\n    def __init__(self, image_size=784, h_dim=400, z_dim=60):\n        super(VAE, self).__init__()\n        self.fc1 = nn.Linear(image_size, 500)\n        self.fc2 = nn.Linear(500, 300)\n        self.fc3 = nn.Linear(300, 150)\n        self.fc4 = nn.Linear(150, z_dim)\n        self.fc41 = nn.Linear(150, z_dim)\n        self.fc5 = nn.Linear(z_dim, 150)\n        self.fc6 = nn.Linear(150, 300)\n        self.fc7 = nn.Linear(300, 500)\n        self.fc8 = nn.Linear(500, image_size)\n        \n    def encode(self, x):\n        h = F.relu(self.fc1(x))\n        h = F.relu(self.fc2(h))\n        h = F.relu(self.fc3(h))\n        return self.fc4(h), self.fc41(h)\n    \n    def reparameterize(self, mu, log_var):\n        std = torch.exp(log_var/2)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        h = F.relu(self.fc5(z))\n        h = F.relu(self.fc6(h))\n        h = F.relu(self.fc7(h))\n        return F.sigmoid(self.fc8(h))\n    \n    def forward(self, x):\n        mu, log_var = self.encode(x)\n        z = self.reparameterize(mu, log_var)\n        x_reconst = self.decode(z)\n        return x_reconst, mu, log_var\n\nmodel = VAE().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Start training\nfor epoch in range(num_epochs):\n    for i, (x, _) in enumerate(data_loader):\n        # Forward pass\n        x = x.to(device).view(-1, image_size)\n        x_reconst, mu, log_var = model(x)\n        \n        # Compute reconstruction loss and kl divergence\n        # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n        \n        # Backprop and optimize\n        loss = reconst_loss + kl_div\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 200 == 0:\n            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}, TOTAL : {:.4f}\" \n                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item(), kl_div.item()+reconst_loss.item()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l[0][0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result for Defense on CW Attack using VAE"},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    \n    correct=0\n    total=0\n    for attacked,trueimages,labels in l:\n        \n        attacked=trueimages.reshape(-1,1,28,28).to(device)\n        labels=labels.to(device)\n        \n        vae_input = attacked.view(-1, image_size)\n        attacked_reconstructed, mu, log_var = model(vae_input)\n        attacked_reconstructed=attacked_reconstructed.view(-1,1,28,28)\n        outputs=my_model(attacked_reconstructed)[0]\n        \n        _,predicted=torch.max(outputs.data,1)\n        total+=labels.size(0)\n        correct+=(predicted==labels).sum().item()\n        print('Accuracy of the network on the attacked images so far: {} %'.format(100 * correct / total))\n        \n    print('Final Accuracy of the network on the attacked images: {} %'.format(100 * correct / total))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result for Defense against Targeted FGSM Attack using VAE"},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    \n    correct=0\n    total=0\n    for attacked,trueimages,labels in l_tfgsm:\n        \n        attacked=trueimages.reshape(-1,1,28,28).to(device)\n        labels=labels.to(device)\n        \n        vae_input = attacked.view(-1, image_size)\n        attacked_reconstructed, mu, log_var = model(vae_input)\n        attacked_reconstructed=attacked_reconstructed.view(-1,1,28,28)\n        outputs=my_model(attacked_reconstructed)[0]\n        \n        _,predicted=torch.max(outputs.data,1)\n        total+=labels.size(0)\n        correct+=(predicted==labels).sum().item()\n        print('Accuracy of the network on the attacked images so far: {} %'.format(100 * correct / total))\n        \n    print('Final Accuracy of the network on the attacked images: {} %'.format(100 * correct / total))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defense VAE using KNN methodology"},{"metadata":{"trusted":true},"cell_type":"code","source":"# l contains values for the CW attack\n# l_tgsm contains the value for the tfgsm attack\n\nfrom sklearn.neighbors import NearestNeighbors\n\nN=1000 #generating N random images from Z space\n\nwith torch.no_grad():\n    # Save the sampled images\n    z = torch.randn(N, z_dim).to(device)\n    output = model.decode(z)  #generation of N images\n    print(output.shape)\n    out= output.view(-1,image_size)\n    \nsamples=out.cpu().numpy()\n#print(numpy_out.shape)\nneigh=NearestNeighbors(n_neighbors=1)\nneigh.fit(samples)\n\ncorrect=0\ntotal=0\n\nfor attacked,trueimages,labels in l:\n    #implementing the K-NN on the attacked\n    \n    temp=attacked.view(-1,image_size).detach()\n    numpy_attacked=temp.cpu().numpy()\n    result=neigh.kneighbors(numpy_attacked)\n    #result[1] is a numpy array containing the index of the closest image in the sample space\n    index=list(result[1].flatten())\n    #print(index)\n    ClosestImage=output[index,:]\n    input_to_classifier=ClosestImage.view(-1,1,28,28)\n    print(input_to_classifier.shape)\n    output=my_model(input_to_classifier)[0]\n    _,predicted=torch.max(outputs.data,1)\n    total+=labels.size(0)\n    correct+=(predicted==labels).sum().item()\n    print('Accuracy of the network on the attacked images so far: {} %'.format(100 * correct / total))\n        \nprint('Final Accuracy of the network on the attacked images: {} %'.format(100 * correct / total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VAE Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    # Save the sampled images\n    z = torch.randn(batch_size, z_dim).to(device)\n    out = model.decode(z).view(-1, 1, 28, 28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    # Save the sampled images\n    z = torch.randn(batch_size, z_dim).to(device)\n    out = model.decode(z).view(-1, 1, 28, 28)\n    \nprint(out.shape)\nout = out.reshape(-1, 28,28).cpu().numpy()\nprint(out.shape)\nimport matplotlib.pyplot as plt\nfrom pylab import * \nrangee = 3\nfor k in range(1, rangee):\n    subplot(2, rangee, k)\n    plt.axis('off')\n    imshow(out[k], cmap='Greys')\n    subplot(2, rangee, k+rangee)\n    imshow(out[k+rangee], cmap='Greys')\n    plt.axis('off')   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(num_epochs):\n    for i, (x, _) in enumerate(data_loader):\n        # Forward pass\n        x = x.to(device).view(-1, image_size)\n        x_reconst, mu, log_var = model(x)\n        \n        # Compute reconstruction loss and kl divergence\n        # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n        \n        # Backprop and optimize\n        loss = reconst_loss + 0.8*kl_div\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 200 == 0:\n            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}, TOTAL : {:.4f}\" \n                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item(), kl_div.item()+reconst_loss.item()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Following are just snippet tests,they don't mean anything as such\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"im,_=l[0]\nim=im.data.cpu().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"old_im,_=actual[0]\nold_im=old_im.data.cpu().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im=im.reshape(100,28,28)\nold_im=old_im.reshape(100,28,28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(old_im[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(im[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#these ae just snippet tests they don't mean anything now\nsubplot(1,2,1)\ntitle('adversarial example')\nimshow(im[2],cmap='gray')\nsubplot(1,2,2)\ntitle('actual sample')\nimshow(old_im[2],cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image1=torch.from_numpy(im[1])\nimg_t1=image1.reshape(-1,1,28,28).to(device)\noutputs=my_model(img_t1)\n_,predicted=torch.max(outputs.data,1)\nprint(predicted)\n\nimage2=torch.from_numpy(old_im[1])\nimg_t2=image2.reshape(-1,1,28,28).to(device)\noutputs=my_model(img_t2)\n_,predicted=torch.max(outputs.data,1)\nprint(predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Training a GAN **\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"g_input_size=10\ng_hidden_size=5\ng_output_size=\n\nd_input_size=\nd_hidden_size=10\nd_output_size=1\nminibatch_size=d_input_size \n\ngenerator_activation_function=torch.tanh\ndiscriminator_activation_function=torch.sigmoid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_generator_input_sampler():\n    return lambda n: torch.Tensor(np.random.normal(0,1,(g_input_size,1)))  #whatever is the size of the random input noise here 50*1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self,input_size,hidden_size,output_size,f):\n        super(Generator,self).__init__()\n        self.map1=nn.Linear(input_size,hidden_size)\n        self.map2=nn.Linear(hidden_size,hidden_size)\n        self.map3=nn.Linear(hidden_size,output_size)\n        self.f=f\n    def forward(self,x):\n        x=self.map1(x)\n        x=self.f(x)\n        x=self.map2(x)\n        x=self.f(x)\n        x=self.map3(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Discriminator(nn.Module):\n    def __init__(self,input_size,hidden_size,output_size,f):\n        super(Discriminator,self).__init__()\n        self.map1=nn.Linear(input_size,hidden_size)\n        self.map2=nn.Linear(hidden_size,hidden_size)\n        self.map3=nn.Linear(hidden_size,output_size)\n        self.f=f\n    def forward(self,x):\n        x=self.f(self.map1(x))\n        x=self.f(self.map2(x))\n        x=self.f(self.map3(x))\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G=Generator(input_size=g_input_size,\n           hidden_size=g_hidden_size,\n           output_size=g_output_size,\n           f=generator_activation_function)\n\nD=Discriminator(input_size=d_input_size,\n           hidden_size=d_hidden_size,\n           output_size=d_output_size,\n           f=discriminator_activation_function)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Number_epochs=50\nD_steps=5\n\nfor epoch in range(Number_epochs):\n    for d_index in range(D_steps):\n        #train D on real+fake\n        D\n        ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}